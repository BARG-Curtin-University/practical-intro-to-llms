<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving
and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
  <front>
    <journal-meta>
      <journal-id/>
      <journal-title-group>
        <journal-title>BARG Curtin University</journal-title>
      </journal-title-group>
      <issn/>
      <publisher>
        <publisher-name/>
      </publisher>
    </journal-meta>
    <article-meta>
      <title-group>
        <article-title>Mastering Large Language Models: A Hands-On Guide to
Practical Applications</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <contrib-id contrib-id-type="orcid">0000-0002-0950-6396</contrib-id>
          <name>
            <surname>Borck</surname>
            <given-names>Michael</given-names>
          </name>
          <string-name>Michael Borck</string-name>
          <email>michael.borck@curtin.edu.au</email>
          <role vocab="https://credit.niso.org" vocab-term="investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role>
          <role vocab="https://credit.niso.org" vocab-term="project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project
administration</role>
          <role vocab="https://credit.niso.org" vocab-term="software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role>
          <role>Visualisation</role>
          <xref ref-type="aff" rid="aff-1">a</xref>
          <xref ref-type="corresp" rid="cor-1">*</xref>
        </contrib>
      </contrib-group>
      <aff id="aff-1">
        <institution-wrap>
          <institution>Business Information Systems, Curtin University, Perth
Australia</institution>
        </institution-wrap>
      </aff>
      <author-notes>
        <corresp id="cor-1">michael.borck@curtin.edu.au</corresp>
      </author-notes>
      <pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-04-25">
        <year>2024</year>
        <month>4</month>
        <day>25</day>
      </pub-date>
      <history/>
      <abstract>
        <p>This tutorial offers a comprehensive introduction to Large Language
Models (LLMs), providing learners with the knowledge and tools needed to
effectively utilise these powerful AI technologies. Through a series of
interactive examples and practical applications, participants will
explore the core concepts of LLMs, including prompt engineering, model
selection, and advanced techniques such as Retrieval-Augmented
Generation (RAG) and text chunking. Designed for both beginners and
experienced users, this guide aims to demystify LLMs and illustrate
their potential in solving real-world problems, enhancing both
understanding and capability in applying cutting-edge AI.</p>
      </abstract>
      <kwd-group kwd-group-type="author">
        <kwd>Large Language Models</kwd>
        <kwd>Practical Applications</kwd>
        <kwd>Text Chunking</kwd>
        <kwd>Retrieval-Augmented Generation (RAG)</kwd>
        <kwd>Vector Databases</kwd>
        <kwd>Prompt Engineering</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="introduction">
      <title>1 Introduction</title>
      <p>Large Language Models (LLMs) represent a significant advancement in
  artificial intelligence, with the ability to understand and generate
  human-like text at an unprecedented scale and complexity. These
  models, exemplified by OpenAI’s GPT series, are characterised by their
  vast number of parameters, allowing them to store and process a large
  amount of linguistic data.</p>
      <p>LLMs exhibit emergent properties such as few-shot learning, where
  they can intuitively perform tasks without explicit prior training.
  This tutorial provides a practical introduction to LLMs, exploring
  their key characteristics and discussing hands-on examples of
  real-world applications.</p>
      <p>Throughout this tutorial, you will learn about fundamental LLM
  tasks such as:</p>
      <list list-type="bullet">
        <list-item>
          <p>Text classification</p>
        </list-item>
        <list-item>
          <p>Sentiment analysis</p>
        </list-item>
        <list-item>
          <p>Question answering</p>
        </list-item>
        <list-item>
          <p>Abstract reasoning</p>
        </list-item>
        <list-item>
          <p>Creative text generation</p>
        </list-item>
      </list>
      <p>We will also delve into the challenges posed by context length
  limitations and introduce strategies like text chunking to mitigate
  these issues. Advanced techniques, including Recursive Abstractive
  Processing for Tree-Organised Retrieval (RAPTOR) and
  Retrieval-Augmented Generation (RAG), will be discussed to further
  enhance LLM capabilities.</p>
      <p>By the end of this tutorial, you will gain a comprehensive
  understanding of LLMs and their practical applications, as well as the
  tools and techniques needed to effectively leverage these powerful
  models.</p>
    </sec>
    <sec id="what-is-an-llm">
      <title>2 What is an LLM?</title>
      <p>Large Language Models, such as OpenAI’s GPT series, represent a
  significant advancement in artificial intelligence. These models are
  capable of understanding and generating text with a level of nuance
  and complexity that closely resembles human output.</p>
    </sec>
    <sec id="key-characteristics-of-llms">
      <title>3 Key Characteristics of LLMs</title>
      <p>LLMs are characterised by their vast number of parameters, allowing
  them to store and process a large amount of linguistic data. These
  models exhibit emergent properties such as sero-shot learning, where
  they can intuitively perform tasks without explicit prior training on
  those tasks.</p>
    </sec>
    <sec id="practical-examples-of-llm-tasks">
      <title>4 Practical Examples of LLM Tasks</title>
      <sec id="text-classification">
        <title>4.1 1. Text Classification</title>
        <p>Text classification involves categorising text into predefined
    categories. For example, classifying news articles into topics such
    as sports, politics, or technology.</p>
        <p><bold>Example</bold>:</p>
        <code language="python">import openai

response = openai.Completion.create(
  engine="text-davinci-003",
  prompt="Classify the following text into categories (sports, politics, technology): 'The government has announced a new initiative to foster innovation in the tech industry.'",
  max_tokens=10
)
print(response.choices[0].text.strip())</code>
      </sec>
      <sec id="sentiment-analysis">
        <title>4.2 2. Sentiment Analysis</title>
        <p>Sentiment analysis is the process of determining the emotional
    tone behind a series of words. This is useful to understand the
    attitudes, opinions, and emotions expressed within an online
    mention.</p>
        <p><bold>Example</bold>:</p>
        <code language="python">import openai

response = openai.Completion.create(
  engine="text-davinci-003",
  prompt="Analyse the sentiment of this review: 'I absolutely loved the new sci-fi movie! The special effects and storyline were breathtaking and left me wanting more.'",
  max_tokens=10
)
print(response.choices[0].text.strip())</code>
      </sec>
      <sec id="question-answering">
        <title>4.3 3. Question Answering</title>
        <p>Question answering systems can comprehend a body of text and
    provide answers to questions based on that text.</p>
        <p><bold>Example</bold>:</p>
        <code language="python">import openai

response = openai.Completion.create(
  engine="text-davinci-003",
  prompt="Given the text: 'The Eiffel Tower is one of the most famous landmarks in the world, located in Paris, France. It was constructed in 1889.' Answer the question: Where is the Eiffel Tower located?",
  max_tokens=10
)
print(response.choices[0].text.strip())</code>
      </sec>
      <sec id="abstract-reasoning">
        <title>4.4 4. Abstract Reasoning</title>
        <p>Abstract reasoning involves understanding complex concepts and
    applying logical thinking to new problems without relying solely on
    factual knowledge.</p>
        <p><bold>Example</bold>:</p>
        <code language="python">import openai

response = openai.Completion.create(
  engine="text-davinci-003",
  prompt="If higher demand for a product generally leads to higher prices, what might happen to the price of a product if a celebrity endorses it positively?",
  max_tokens=50
)
print(response.choices[0].text.strip())</code>
      </sec>
      <sec id="creative-text-generation">
        <title>4.5 5. Creative Text Generation</title>
        <p>Creative text generation can involve writing stories, poems, or
    generating any form of text that requires imagination and
    creativity.</p>
        <p><bold>Example</bold>:</p>
        <code language="python">import openai

response = openai.Completion.create(
  engine="text-davinci-003",
  prompt="Write a short story about a robot discovering a hidden ancient civilisation on Mars.",
  max_tokens=200
)
print(response.choices[0].text.strip())</code>
      </sec>
      <sec id="implementation-notes">
        <title>4.6 Implementation Notes</title>
        <list list-type="bullet">
          <list-item>
            <p><bold>API Access</bold>: To run these examples, you need
        access to an API like OpenAI’s GPT-3. Make sure you have the
        necessary API keys and understand the usage costs associated
        with these requests.</p>
          </list-item>
          <list-item>
            <p><bold>Prompt Design</bold>: The effectiveness of an LLM in
        performing these tasks often hinges on how the prompt is
        structured. Clear, concise, and well-directed prompts tend to
        yield better results.</p>
          </list-item>
          <list-item>
            <p><bold>Model Choice</bold>: Depending on your specific needs
        (e.g., detail of response, cost, latency), you might choose
        different models. OpenAI offers a range of models from the more
        cost-effective Ada to the more powerful and detailed
        Davinci.</p>
          </list-item>
        </list>
        <p>These examples showcase the breadth of applications for LLMs
    across various domains, emphasising their role as a transformative
    technology in the field of AI.</p>
      </sec>
    </sec>
    <sec id="understanding-context-length-limitations">
      <title>5 Understanding Context Length Limitations</title>
      <sec id="the-challenge-of-context-length">
        <title>5.1 The Challenge of Context Length</title>
        <p>Context length refers to the maximum number of tokens an LLM can
    process in a single prompt. This limitation can significantly impact
    the model’s performance, particularly for tasks requiring a deep
    understanding of long documents or complex queries.</p>
      </sec>
      <sec id="impact-and-examples">
        <title>5.2 Impact and Examples</title>
        <p>For instance, when asked to summarise a long article, an LLM
    might only consider the first section if the article exceeds the
    model’s token limit, potentially omitting key details found later in
    the text.</p>
        <list list-type="bullet">
          <list-item>
            <p>[ADD EXAMPLE OF CONTEXT LENGTH LIMITATION HERE]</p>
          </list-item>
        </list>
      </sec>
      <sec id="text-chunking-strategies">
        <title>5.3 Text Chunking Strategies</title>
        <p>To mitigate the impact of context length limitations, text
    chunking strategies can be employed. Chunking involves dividing a
    long document into smaller, discrete pieces that fit within the
    model’s maximum token count. Each chunk is then processed
    independently, and the results can be aggregated or further
    processed to derive a comprehensive understanding or response.</p>
        <sec id="implementing-text-chunking">
          <title>5.3.1 Implementing Text Chunking</title>
          <p>Here are some approaches to chunking:</p>
          <list list-type="bullet">
            <list-item>
              <p><bold>Equal Division</bold>: Split the text into equally
          sized chunks that do not exceed the token limit. Care must be
          taken to ensure that the division does not cut sentences or
          important semantic units inappropriately.</p>
            </list-item>
            <list-item>
              <p><bold>Semantic Preservation</bold>: Use natural linguistic
          breaks such as sentences, paragraphs, or sections to divide
          the text. This method helps maintain the coherence of the
          information in each chunk.</p>
            </list-item>
            <list-item>
              <p><bold>Overlap Strategy</bold>: Include some overlap between
          consecutive chunks to preserve context continuity. This can
          help in maintaining the flow and coherence across chunks,
          especially for tasks like summarisation or detailed
          analysis.</p>
            </list-item>
            <list-item>
              <p>[ADD CODE EXAMPLE FOR TEXT CHUNKING HERE]</p>
            </list-item>
          </list>
        </sec>
      </sec>
    </sec>
    <sec id="advanced-techniques">
      <title>6 Advanced Techniques</title>
      <p>While prompt engineering and chunking are useful, it may not always
  suffice for tasks requiring deep integration of information across a
  long text. In such cases, advanced techniques like Recursive
  Abstractive Processing for Tree-Organised Retrieval (RAPTOR) and
  Retrieval-Augmented Generation (RAG) can further enhance the
  capability of LLMs to process extensive data by pulling in relevant
  information dynamically as needed.</p>
      <sec id="bridging-the-gap-with-raptor">
        <title>6.1 Bridging the Gap with RAPTOR</title>
        <p>RAPTOR is a novel retrieval method that aims to enhance the
    performance of large language models (LLMs) by providing a more
    effective way to handle long context. RAPTOR builds a hierarchical
    tree structure by recursively clustering and summarising text chunks
    from the retrieval corpus4. This allows LLMs to access relevant
    information at different levels of specificity, from low-level
    details to high-level summaries. In summary, RAPTOR is a promising
    approach to enhance LLMs by providing a more effective retrieval
    mechanism that can handle long context and capture high-level and
    low-level details of the text[3][4]. The tree-based structure
    enables LLMs to integrate knowledge from multiple parts of the text,
    leading to improved performance on various tasks.</p>
        <list list-type="bullet">
          <list-item>
            <p>[ADD CODE EXAMPLE FOR RAPTOR HERE]</p>
          </list-item>
        </list>
      </sec>
      <sec id="bridging-the-gap-with-rag">
        <title>6.2 Bridging the Gap with RAG</title>
        <p>Retrieval-Augmented Generation enhances LLM capabilities by
    integrating a retrieval system that fetches relevant external
    information to supplement the model’s responses. This technique
    effectively extends the LLM’s ability to handle queries that exceed
    its native context length.</p>
      </sec>
      <sec id="introduction-to-vector-databases">
        <title>6.3 Introduction to Vector Databases</title>
        <p>Vector databases store data as vectors of real numbers, which
    represent different features or aspects of the data items. These
    databases are crucial in the context of LLMs for efficiently storing
    and retrieving large amounts of vectorised data, such as text
    embeddings. They are particularly useful for tasks involving
    semantic search, where the goal is to find items in a dataset that
    are semantically similar to a query.</p>
      </sec>
      <sec id="the-need-for-rag-and-vector-databases">
        <title>6.4 The Need for RAG and Vector Databases</title>
        <p>Retrieval-Augmented Generation (RAG) combines the generative
    capabilities of LLMs with a retrieval mechanism that fetches
    relevant information to support the generation process. Here, vector
    databases play a critical role by enabling quick retrieval of the
    most relevant text segments or documents based on their semantic
    similarity to the query. This is essential for ensuring that the
    generative model has access to the most contextually appropriate
    information.</p>
      </sec>
      <sec id="text-chunking-and-embeddings">
        <title>6.5 Text Chunking and Embeddings</title>
        <p>Before leveraging a vector database for RAG, it is often
    necessary to preprocess the data, especially when dealing with large
    texts. Text chunking becomes a preparatory step here:</p>
        <list list-type="order">
          <list-item>
            <p><bold>Chunking Large Texts</bold>: Divide long documents into
        smaller segments or chunks that are more manageable for
        processing by LLMs. This helps to ensure that each piece of text
        can be individually analysed and vectorised without losing
        meaning due to truncation.</p>
          </list-item>
          <list-item>
            <p><bold>Creating Embeddings</bold>: Once the texts are chunked,
        each piece is converted into a vector representation (embedding)
        that captures its semantic essence. These embeddings are then
        stored in a vector database.</p>
          </list-item>
          <list-item>
            <p><bold>Utilising Vector Databases</bold>: When a query is
        made, the LLM retrieves the embeddings that are most
        semantically similar to the query from the vector database.
        These embeddings represent chunks of text that are most relevant
        to the query, which the LLM then uses to generate an informed
        response.</p>
          </list-item>
        </list>
        <sec id="implementation-of-text-chunking-and-embeddings">
          <title>6.5.1 Implementation of Text Chunking and
      Embeddings</title>
          <list list-type="bullet">
            <list-item>
              <p><bold>Chunking Strategy</bold>: Implement a strategy that
          respects natural language breaks and includes overlap for
          context continuity.</p>
            </list-item>
            <list-item>
              <p><bold>Embedding Process</bold>: Use a model like BERT or
          RoBERTa to convert text chunks into embeddings.</p>
            </list-item>
            <list-item>
              <p><bold>Storing Embeddings</bold>: Store these embeddings in
          a vector database like Faiss, Elasticsearch, or Annoy, which
          facilitates fast and efficient retrieval.</p>
            </list-item>
            <list-item>
              <p>[ADD CODE EXAMPLE FOR TEXT CHUNKING AND EMBEDDING HERE]</p>
            </list-item>
          </list>
        </sec>
      </sec>
      <sec id="combining-rag-with-vector-databases">
        <title>6.6 Combining RAG with Vector Databases</title>
        <p>Incorporating RAG with vector databases optimises the retrieval
    process, ensuring that the generative component of the LLM has
    access to the most relevant and comprehensive information available.
    This technique significantly enhances the LLM’s ability to generate
    accurate and contextually relevant responses, particularly for
    complex queries that go beyond the model’s training data.</p>
        <list list-type="bullet">
          <list-item>
            <p>[ADD CODE EXAMPLE FOR RAG WITH VECTOR DATABASES HERE]</p>
          </list-item>
        </list>
      </sec>
      <sec id="implementing-rag">
        <title>6.7 Implementing RAG</title>
        <p>RAG can be particularly useful in scenarios requiring up-to-date
    information or responses based on extensive data not contained
    within the model’s initial training set.</p>
        <list list-type="bullet">
          <list-item>
            <p>[ADD CODE EXAMPLE FOR RAG IMPLEMENTATION HERE]</p>
          </list-item>
        </list>
      </sec>
      <sec id="implementing-rag-without-a-vector-database">
        <title>6.8 Implementing RAG Without a Vector Database</title>
        <p>It is possible to implement Retrieval-Augmented Generation (RAG)
    without explicitly using a vector database managing and querying
    large volumes of data or embeddings can become inefficient as the
    dataset grows. Memory limitations and slower retrieval times are
    potential challenges. Instead of using a vector database to store
    and retrieve embeddings, you can directly search through a set of
    documents or data. This approach involves:</p>
        <list list-type="bullet">
          <list-item>
            <p><bold>Document Storage</bold>: Documents are stored in their
        raw textual form, possibly indexed using a traditional text
        search engine like Elasticsearch, which supports full-text
        search capabilities without necessarily using vector
        embeddings.</p>
          </list-item>
          <list-item>
            <p><bold>Query Matching</bold>: When a query is issued, the
        system performs a text search to find documents or snippets that
        match the query based on keyword similarity or other traditional
        search metrics.</p>
          </list-item>
          <list-item>
            <p><bold>Data Retrieval</bold>: The most relevant documents or
        snippets are then retrieved based on the search results and
        passed to the LLM for processing.</p>
          </list-item>
          <list-item>
            <p><bold>Embedding Storage</bold>: Embeddings are precomputed
        and stored in an array or similar data structure in the
        application’s memory.</p>
          </list-item>
          <list-item>
            <p><bold>Similarity Computation</bold>: When a query is
        processed, its embedding is compared against the stored
        embeddings using similarity metrics (like cosine
        similarity).</p>
          </list-item>
          <list-item>
            <p><bold>Selection of Top Results</bold>: The top N most similar
        embeddings are selected, and the corresponding text chunks are
        fed into the LLM for generating the final output.</p>
          </list-item>
        </list>
        <p>If not using a vector database, consider leveraging:</p>
        <list list-type="bullet">
          <list-item>
            <p><bold>Full-Text Search Engines</bold>: Tools like
        Elasticsearch or Apache Solr that support advanced text indexing
        and search capabilities.</p>
          </list-item>
          <list-item>
            <p><bold>In-Memory Databases</bold>: If dataset size allows,
        in-memory databases or caching solutions can store embeddings
        and support relatively fast retrieval operations.</p>
          </list-item>
        </list>
        <code language="python"># Example placeholder for RAG implementation using a full-text search engine
# [ADD CODE EXAMPLE FOR RAG USING FULL-TEXT SEARCH HERE]</code>
        <p>Implementing RAG without a vector database is feasible,
    especially for smaller or less complex datasets. However, for
    applications that require handling large datasets with a need for
    quick and semantically rich retrieval, using a vector database
    remains a more effective and scalable solution.</p>
      </sec>
    </sec>
    <sec id="levels-of-using-llms-in-practice">
      <title>7 Levels of Using LLMs in Practice</title>
      <p>This section outlines a progression of techniques from basic to
  advanced, helping users understand and effectively utilise Large
  Language Models (LLMs) across a range of tasks.</p>
      <sec id="level-1-prompt-engineering">
        <title>7.1 Level 1: Prompt Engineering</title>
        <p><bold>Description</bold>: This level involves using LLMs
    “out-of-the-box” without modifying any underlying model parameters.
    It’s the simplest and most accessible form of interaction with
    LLMs.</p>
        <p><bold>Applications</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p>Generating text based on simple prompts.</p>
          </list-item>
          <list-item>
            <p>Answering straightforward questions or performing basic tasks
        like text classification.</p>
          </list-item>
        </list>
        <p><bold>Example</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p>[ADD CODE EXAMPLE FOR BASIC PROMPT ENGINEERING HERE]</p>
          </list-item>
        </list>
      </sec>
      <sec id="level-2-text-chunking-and-basic-retrieval">
        <title>7.2 Level 2: Text Chunking and Basic Retrieval</title>
        <p><bold>Description</bold>: As users encounter the limitations of
    LLMs due to context length, text chunking and basic retrieval
    techniques can be employed to manage and process larger documents
    effectively.</p>
        <p><bold>Applications</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p>Breaking down long documents into manageable pieces.</p>
          </list-item>
          <list-item>
            <p>Using simple retrieval methods to fetch relevant text chunks
        based on keyword search or basic semantic matching.</p>
          </list-item>
        </list>
        <p><bold>Example</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p>[ADD CODE EXAMPLE FOR TEXT CHUNKING AND BASIC RETRIEVAL
        HERE]</p>
          </list-item>
        </list>
      </sec>
      <sec id="level-3-raptor---advanced-hierarchical-retrieval">
        <title>7.3 Level 3: RAPTOR - Advanced Hierarchical Retrieval</title>
        <p><bold>Description</bold>: RAPTOR enhances LLMs by building a
    hierarchical tree structure that clusters and summarises text chunks
    from the retrieval corpus. This method allows for accessing
    information at various levels of specificity, from detailed extracts
    to broad summaries.</p>
        <p><bold>Applications</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p>Enhancing the model’s ability to handle long contexts by
        allowing it to access relevant information without being
        overwhelmed by data volume.</p>
          </list-item>
          <list-item>
            <p>Improving performance on tasks that require integration of
        knowledge from multiple text parts, such as comprehensive
        analyses or detailed explorations of topics.</p>
          </list-item>
        </list>
        <p><bold>Example</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p>[ADD CODE EXAMPLE FOR RAPTOR IMPLEMENTATION HERE]</p>
          </list-item>
        </list>
      </sec>
      <sec id="level-4-retrieval-augmented-generation-rag-with-vector-databases">
        <title>7.4 Level 4: Retrieval-Augmented Generation (RAG) with Vector
    Databases</title>
        <p><bold>Description</bold>: This level introduces the integration
    of LLMs with vector databases for dynamic information retrieval,
    enhancing the model’s ability to generate responses based on
    comprehensive and semantically relevant external data.</p>
        <p><bold>Applications</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p>Complex question answering where responses require up-to-date
        or detailed knowledge.</p>
          </list-item>
          <list-item>
            <p>Tasks that benefit from access to a broader range of
        information than what is contained in the model’s training
        data.</p>
          </list-item>
        </list>
        <p><bold>Example</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p>[ADD CODE EXAMPLE FOR RAG WITH VECTOR DATABASES HERE]</p>
          </list-item>
        </list>
      </sec>
      <sec id="level-5-advanced-rag-and-fine-tuning">
        <title>7.5 Level 5: Advanced RAG and Fine-Tuning</title>
        <p><bold>Description</bold>: At this level, users not only utilise
    advanced RAG setups but also start fine-tuning models to specific
    datasets or tasks, optimising performance for particular use
    cases.</p>
        <p><bold>Applications</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p>Tailoring responses more closely to specific domain
        requirements.</p>
          </list-item>
          <list-item>
            <p>Enhancing accuracy and relevance in scenarios where standard
        model outputs require adaptation to unique contexts.</p>
          </list-item>
        </list>
        <p><bold>Example</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p>[ADD CODE EXAMPLE FOR FINE-TUNING LLMs HERE]</p>
          </list-item>
        </list>
      </sec>
      <sec id="level-6-building-custom-llms-from-scratch">
        <title>7.6 Level 6: Building Custom LLMs from Scratch</title>
        <p><bold>Description</bold>: The most advanced level involves
    developing entirely new LLMs, customised from the ground up. This
    approach requires significant resources and expertise but offers the
    highest degree of customisation.</p>
        <p><bold>Applications</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p>Developing LLMs for specialised fields, such as legal or
        medical domains, where existing models may not provide
        sufficient accuracy.</p>
          </list-item>
          <list-item>
            <p>Creating models tailored to unique linguistic or cultural
        contexts not well-represented in general-purpose models.</p>
          </list-item>
        </list>
        <p><bold>Example</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p>[ADD CODE EXAMPLE FOR BUILDING LLMs FROM SCRATCH HERE]</p>
          </list-item>
        </list>
        <p>Absolutely, adding a section about the resources required for
    each level can provide valuable context for readers, helping them
    assess the feasibility of engaging with these technologies based on
    their own capabilities and resources. This section can also guide
    readers towards achievable goals within their means and encourage
    exploration of lower levels or alternative approaches that are more
    accessible.</p>
      </sec>
    </sec>
    <sec id="resource-requirements-and-accessibility">
      <title>8 Resource Requirements and Accessibility</title>
      <p>Understanding the resource requirements for each level of LLM usage
  can help set realistic expectations and drive informed
  decision-making. This section provides insights into the technical,
  financial, and human resources needed for each level, offering
  alternatives where possible. This section helps you gauge what’s
  involved at each level but also encourages them to consider
  alternative, less resource-intensive ways to participate in the
  development and application of LLM technologies.</p>
      <sec id="level-1-prompt-engineering-1">
        <title>8.1 Level 1: Prompt Engineering</title>
        <p><bold>Resources Needed</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p><bold>Technical</bold>: Minimal technical skills required;
        basic understanding of how to interact with APIs.</p>
          </list-item>
          <list-item>
            <p><bold>Financial</bold>: Low cost; many platforms offer free
        tiers or inexpensive options for light usage.</p>
          </list-item>
          <list-item>
            <p><bold>Human</bold>: Individual researchers, developers, or
        hobbyists can easily manage.</p>
          </list-item>
        </list>
        <p><bold>Motivation</bold>: Ideal for individuals or small teams
    looking to explore LLM capabilities without significant
    investment.</p>
      </sec>
      <sec id="level-2-text-chunking-and-basic-retrieval-1">
        <title>8.2 Level 2: Text Chunking and Basic Retrieval</title>
        <p><bold>Resources Needed</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p><bold>Technical</bold>: Moderate technical skills;
        familiarity with basic programming and data handling.</p>
          </list-item>
          <list-item>
            <p><bold>Financial</bold>: Generally low cost; dependent on the
        volume of data processed.</p>
          </list-item>
          <list-item>
            <p><bold>Human</bold>: Accessible to small teams or educational
        settings.</p>
          </list-item>
        </list>
        <p><bold>Motivation</bold>: Encourages deeper understanding of data
    preprocessing and simple retrieval methods that enhance LLM
    outputs.</p>
      </sec>
      <sec id="level-3-raptor---advanced-hierarchical-retrieval-1">
        <title>8.3 Level 3: RAPTOR - Advanced Hierarchical Retrieval</title>
        <p><bold>Resources Needed</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p><bold>Technical</bold>: Advanced programming skills and
        understanding of machine learning principles.</p>
          </list-item>
          <list-item>
            <p><bold>Financial</bold>: Moderate cost for implementing and
        maintaining hierarchical systems.</p>
          </list-item>
          <list-item>
            <p><bold>Human</bold>: Suitable for research groups or companies
        with dedicated R&amp;D capabilities.</p>
          </list-item>
        </list>
        <p><bold>Motivation</bold>: Explore cutting-edge retrieval
    techniques that significantly improve the contextual handling of
    LLMs.</p>
      </sec>
      <sec id="level-4-retrieval-augmented-generation-with-vector-databases">
        <title>8.4 Level 4: Retrieval-Augmented Generation with Vector
    Databases</title>
        <p><bold>Resources Needed</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p><bold>Technical</bold>: High level of expertise in machine
        learning, databases, and system integration.</p>
          </list-item>
          <list-item>
            <p><bold>Financial</bold>: Significant investment required for
        infrastructure and ongoing operation.</p>
          </list-item>
          <list-item>
            <p><bold>Human</bold>: Best suited for well-resourced
        organisations or collaborative research projects.</p>
          </list-item>
        </list>
        <p><bold>Motivation</bold>: Engage in advanced LLM applications that
    require dynamic, real-time data retrieval and processing.</p>
      </sec>
      <sec id="level-5-advanced-rag-and-fine-tuning-1">
        <title>8.5 Level 5: Advanced RAG and Fine-Tuning</title>
        <p><bold>Resources Needed</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p><bold>Technical</bold>: Deep expertise in NLP and machine
        learning model training.</p>
          </list-item>
          <list-item>
            <p><bold>Financial</bold>: High costs for training data,
        computing power, and model maintenance.</p>
          </list-item>
          <list-item>
            <p><bold>Human</bold>: Typically requires a team of experts and
        significant organisational support.</p>
          </list-item>
        </list>
        <p><bold>Motivation</bold>: Tailor LLM outputs to specific needs,
    enhancing relevance and accuracy for specialised applications.</p>
      </sec>
      <sec id="level-6-building-custom-llms-from-scratch-1">
        <title>8.6 Level 6: Building Custom LLMs from Scratch</title>
        <p><bold>Resources Needed</bold>:</p>
        <list list-type="bullet">
          <list-item>
            <p><bold>Technical</bold>: Extensive expertise in AI, machine
        learning, software development, and data science.</p>
          </list-item>
          <list-item>
            <p><bold>Financial</bold>: Substantial funding necessary for
        data acquisition, computing resources, and personnel.</p>
          </list-item>
          <list-item>
            <p><bold>Human</bold>: Requires a large team of specialists,
        often within a corporate or large-scale academic
        environment.</p>
          </list-item>
        </list>
        <p><bold>Motivation</bold>: Develop highly specialised models for
    unique or innovative applications, pushing the boundaries of what
    LLMs can achieve.</p>
      </sec>
      <sec id="encouraging-accessible-alternatives">
        <title>8.7 Encouraging Accessible Alternatives</title>
        <p>For those with limited resources, focusing on quantised models or
    exploring stable diffusion techniques might offer a more viable
    entry point. These approaches allow for the customisation and
    enhancement of LLM capabilities without the need for extensive
    resources:</p>
        <list list-type="bullet">
          <list-item>
            <p><bold>Quantised Models</bold>: Reduced resource consumption
        and potentially lower costs while maintaining reasonable
        performance.</p>
          </list-item>
          <list-item>
            <p><bold>Mixture of Experts</bold>: Leveraging specialised,
        lightweight models that can be combined to address specific
        tasks effectively.</p>
          </list-item>
        </list>
      </sec>
    </sec>
    <sec id="limitations-and-challenges-of-large-language-models-llms">
      <title>9 Limitations and Challenges of Large Language Models
  (LLMs)</title>
      <p>While LLMs have demonstrated impressive capabilities in various
  natural language processing tasks, they also face several limitations
  and challenges that need to be considered when deploying these models
  in real-world applications.</p>
      <sec id="lack-of-commonsense-reasoning">
        <title>9.1 1. <bold>Lack of Commonsense Reasoning</bold></title>
        <p>LLMs can struggle with tasks that require commonsense reasoning,
    as they may lack the ability to draw inferences based on real-world
    knowledge. This limitation can lead to inconsistent or nonsensical
    outputs in certain contexts.</p>
      </sec>
      <sec id="potential-for-biased-and-harmful-outputs">
        <title>9.2 2. <bold>Potential for Biased and Harmful
    Outputs</bold></title>
        <p>LLMs can perpetuate and amplify societal biases present in their
    training data, leading to biased and potentially harmful outputs.
    This is a significant concern when using these models in high-stakes
    applications such as healthcare or finance.</p>
      </sec>
      <sec id="difficulty-in-interpreting-model-outputs">
        <title>9.3 3. <bold>Difficulty in Interpreting Model
    Outputs</bold></title>
        <p>The inner workings of LLMs are often opaque, making it
    challenging to interpret and explain their outputs. This lack of
    transparency can hinder trust and accountability when using these
    models in critical applications.</p>
      </sec>
      <sec id="potential-for-hallucination-and-factual-inaccuracies">
        <title>9.4 4. <bold>Potential for Hallucination and Factual
    Inaccuracies</bold></title>
        <p>LLMs can sometimes generate plausible-sounding but factually
    incorrect information, a phenomenon known as hallucination. This can
    be particularly problematic in applications that require accurate
    and reliable information.</p>
      </sec>
      <sec id="lack-of-long-term-memory-and-consistency">
        <title>9.5 5. <bold>Lack of Long-Term Memory and
    Consistency</bold></title>
        <p>LLMs typically operate on a per-input basis and do not maintain
    long-term memory or consistency across multiple interactions. This
    can lead to inconsistent outputs and make it difficult to engage in
    coherent, long-term conversations or tasks.</p>
      </sec>
      <sec id="computational-and-memory-limitations">
        <title>9.6 6. <bold>Computational and Memory
    Limitations</bold></title>
        <p>LLMs require significant computational resources and memory to
    operate effectively. This can limit their scalability and make them
    challenging to deploy in resource-constrained environments.</p>
      </sec>
      <sec id="difficulty-in-handling-out-of-distribution-data">
        <title>9.7 7. <bold>Difficulty in Handling Out-of-Distribution
    Data</bold></title>
        <p>LLMs may struggle with inputs that are significantly different
    from their training data, leading to unpredictable or unreliable
    outputs. This can be a concern when using these models in dynamic or
    rapidly evolving environments.</p>
        <p>To address these limitations and challenges, ongoing research is
    exploring techniques such as improved training data curation, model
    architecture modifications, and the development of more robust and
    interpretable language models. However, it is crucial for users to
    be aware of these limitations and to carefully evaluate the
    suitability of LLMs for their specific use cases.</p>
      </sec>
    </sec>
    <sec id="ethical-considerations-and-potential-risks-of-large-language-models-llms">
      <title>10 Ethical Considerations and Potential Risks of Large Language
  Models (LLMs)</title>
      <p>As Large Language Models (LLMs) continue to advance and become more
  widely adopted, it is crucial to consider the ethical implications and
  potential risks associated with these powerful technologies. Some key
  ethical considerations and risks include:</p>
      <sec id="bias-and-fairness">
        <title>10.1 1. <bold>Bias and Fairness</bold></title>
        <p>LLMs can perpetuate and amplify societal biases present in their
    training data, leading to biased outputs that discriminate against
    certain groups. This is a significant concern when using these
    models in high-stakes applications such as hiring, lending, or
    criminal justice.</p>
      </sec>
      <sec id="privacy-and-data-rights">
        <title>10.2 2. <bold>Privacy and Data Rights</bold></title>
        <p>The training of LLMs often involves the use of large datasets
    containing personal information and copyrighted material. Ensuring
    the privacy of individuals and respecting intellectual property
    rights is essential when developing and deploying these models.</p>
      </sec>
      <sec id="transparency-and-explainability">
        <title>10.3 3. <bold>Transparency and Explainability</bold></title>
        <p>The inner workings of LLMs are often opaque, making it
    challenging to interpret and explain their outputs. This lack of
    transparency can hinder trust, accountability, and the ability to
    identify and mitigate potential harms.</p>
      </sec>
      <sec id="misuse-and-malicious-applications">
        <title>10.4 4. <bold>Misuse and Malicious
    Applications</bold></title>
        <p>LLMs can be used to generate fake content, impersonate real
    people, or create disinformation at scale. This raises concerns
    about the potential for misuse in areas such as fraud, manipulation,
    and the erosion of trust in online information.</p>
      </sec>
      <sec id="accountability">
        <title>10.5 5. <bold>Accountability</bold></title>
        <p>It is important to ensure that there are mechanisms in place to
    hold entities accountable for the consequences of deploying LLMs.
    Establish clear guidelines on the responsible use of LLMs and adhere
    to relevant laws and regulations. Ensure human oversight in
    decision-making processes involving critical applications to prevent
    over-reliance on automated systems.</p>
      </sec>
      <sec id="security">
        <title>10.6 6. <bold>Security</bold></title>
        <p>Securing LLMs against malicious uses and ensuring the integrity
    of the models are important to prevent misuse. Implement robust
    security measures to protect models from adversarial attacks and
    unauthorised access. Conduct regular security audits to identify and
    address vulnerabilities.</p>
      </sec>
      <sec id="environmental-impact">
        <title>10.7 7. <bold>Environmental Impact</bold></title>
        <p>Training large-scale models is resource-intensive and has a
    significant carbon footprint. Considerations for reducing
    environmental impact include: Utilise energy-efficient computing
    resources to minimise the environmental impact. Invest in renewable
    energy and carbon offset programs to mitigate the emissions
    associated with model training and deployment.</p>
      </sec>
      <sec id="displacement-of-human-labor">
        <title>10.8 8. <bold>Displacement of Human Labor</bold></title>
        <p>As LLMs become more capable of performing tasks traditionally
    done by humans, there are concerns about the potential displacement
    of certain types of jobs. This could lead to economic disruption and
    the need for workforce retraining and adaptation.</p>
      </sec>
      <sec id="existential-risk">
        <title>10.9 9. <bold>Existential Risk</bold></title>
        <p>Some experts worry that as AI systems like LLMs become more
    advanced, they could pose existential risks to humanity if not
    developed and deployed responsibly. This concern highlights the
    importance of aligning these technologies with human values and
    ensuring they remain under human control.</p>
        <p>To address these ethical considerations and mitigate potential
    risks, ongoing research is exploring techniques such as improved
    training data curation, model architecture modifications, and the
    development of more robust and interpretable language models.
    Additionally, the AI community is actively engaged in developing
    ethical frameworks, guidelines, and best practices for the
    responsible development and use of LLMs.</p>
        <p>It is crucial for developers, researchers, and users of LLMs to
    be aware of these ethical considerations and to carefully evaluate
    the potential impacts and risks associated with these technologies.
    By proactively addressing these concerns, we can work towards
    harnessing the power of LLMs in a way that maximises their benefits
    while minimising potential harms to individuals and society.</p>
      </sec>
    </sec>
    <sec id="conclusion">
      <title>11 Conclusion</title>
      <p>This tutorial has laid the groundwork for understanding and
  utilising Large Language Models. We have introduced basic concepts and
  practical applications, discussed limitations, and previewed advanced
  techniques. In subsequent tutorials, we will delve deeper into these
  topics, enhancing our understanding and capabilities with LLMs.</p>
      <sec id="further-reading-and-resources">
        <title>11.1 Further Reading and Resources</title>
        <p>To deepen your understanding of Large Language Models (LLMs) and
    stay updated with the latest advancements in the field, a variety of
    resources are available. Here are some recommended readings and
    resources that can help expand your knowledge and skills in working
    with LLMs.</p>
        <sec id="books">
          <title>11.1.1 Books</title>
          <list list-type="bullet">
            <list-item>
              <p><bold>“Deep Learning for Natural Language
          Processing”</bold> - This book provides a comprehensive
          overview of deep learning techniques used in natural language
          processing, including the foundational concepts behind
          LLMs.</p>
            </list-item>
            <list-item>
              <p><bold>“Artificial Intelligence: A Guide for Thinking
          Humans”</bold> - This book offers a critical examination of
          the capabilities and limitations of current AI technologies,
          including detailed discussions on LLMs.</p>
            </list-item>
            <list-item>
              <p>“The Hundred-Page Machine Learning Book” by Andriy
          Burkov</p>
            </list-item>
            <list-item>
              <p>“Natural Language Processing with Transformers” by Lewis
          Tunstall, Leandro von Werra, and Antonio Torrejon</p>
            </list-item>
            <list-item>
              <p>“Hands-On Machine Learning with Scikit-Learn, Keras, and
          TensorFlow” by Aurélien Géron</p>
            </list-item>
          </list>
        </sec>
        <sec id="research-papers">
          <title>11.1.2 Research Papers</title>
          <list list-type="bullet">
            <list-item>
              <p><bold>“Attention Is All You Need” by Vaswani et al.</bold>
          - Introducing the transformer model, this paper is fundamental
          to understanding the architecture underlying most modern
          LLMs.</p>
            </list-item>
            <list-item>
              <p><bold>“Language Models are Few-Shot Learners” by Brown et
          al. (OpenAI)</bold> - This paper details the methodology and
          capabilities of GPT-3, providing insights into the workings
          and potential applications of LLMs.</p>
            </list-item>
            <list-item>
              <p>“Attention is All You Need” by Ashish Vaswani et al.</p>
            </list-item>
            <list-item>
              <p>“BERT: Pre-training of Deep Bidirectional Transformers for
          Language Understanding” by Jacob Devlin et al.</p>
            </list-item>
            <list-item>
              <p>“GPT-3: Language Models are Few-Shot Learners” by Tom B.
          Brown et al.</p>
            </list-item>
          </list>
        </sec>
        <sec id="online-courses">
          <title>11.1.3 Online Courses</title>
          <list list-type="bullet">
            <list-item>
              <p><bold>Coursera: Neural Networks and Deep Learning</bold> -
          This course offers beginners a deep dive into the neural
          networks that power LLMs.</p>
            </list-item>
            <list-item>
              <p><bold>Udacity: Natural Language Processing
          Nanodegree</bold> - For those looking for practical, hands-on
          training in NLP technologies, including the use of LLMs.</p>
            </list-item>
            <list-item>
              <p>“Natural Language Processing (NLP) Specialisation” by
          deeplearning.ai on Coursera</p>
            </list-item>
            <list-item>
              <p>“Machine Learning” by Andrew Ng on Coursera</p>
            </list-item>
            <list-item>
              <p>“CS224n: Natural Language Processing with Deep Learning” by
          Stanford University on YouTube</p>
            </list-item>
          </list>
        </sec>
        <sec id="tutorials-and-guides">
          <title>11.1.4 Tutorials and Guides</title>
          <list list-type="bullet">
            <list-item>
              <p><bold>OpenAI’s GPT-3 Sandbox</bold> - OpenAI provides a
          platform where developers can experiment with GPT-3 to
          understand its capabilities and limitations.</p>
            </list-item>
            <list-item>
              <p><bold>Hugging Face’s Transformer Models</bold> - A
          comprehensive guide and toolkit for implementing transformer
          models, including several pre-trained LLMs that can be
          customised and deployed.</p>
            </list-item>
            <list-item>
              <p>“The Illustrated Transformer” by Jay Alammar</p>
            </list-item>
            <list-item>
              <p>“Hugging Face’s Transformers: State of the Art Natural
          Language Processing” by Patrick von Platen</p>
            </list-item>
            <list-item>
              <p>“The Annotated Transformer” by Alexander Rush</p>
            </list-item>
          </list>
        </sec>
        <sec id="conferences">
          <title>11.1.5 Conferences</title>
          <list list-type="bullet">
            <list-item>
              <p><bold>NeurIPS (Neural Information Processing
          Systems)</bold> - Annual conference featuring the latest
          research in neural networks, including sessions dedicated to
          LLMs.</p>
            </list-item>
            <list-item>
              <p><bold>ACL (Association for Computational
          Linguistics)</bold> - This conference focuses specifically on
          advancements in NLP and often features sessions on the latest
          developments in LLMs.</p>
            </list-item>
          </list>
        </sec>
        <sec id="forums-and-community-groups">
          <title>11.1.6 Forums and Community Groups</title>
          <list list-type="bullet">
            <list-item>
              <p><bold>Stack Overflow</bold> - A vital resource for
          troubleshooting and community advice on implementing and
          optimising LLMs.</p>
            </list-item>
            <list-item>
              <p><bold>Reddit r/MachineLearning</bold> - A community where
          enthusiasts and professionals discuss the latest trends and
          challenges in machine learning, including LLMs.</p>
            </list-item>
          </list>
        </sec>
        <sec id="podcasts">
          <title>11.1.7 Podcasts</title>
          <list list-type="bullet">
            <list-item>
              <p><bold>The AI Alignment Podcast</bold> - Features
          discussions with researchers and industry leaders focused on
          the future of AI and ethical considerations.</p>
            </list-item>
            <list-item>
              <p><bold>NLP Highlights</bold> - Regular podcast episodes that
          discuss recent papers and trends in natural language
          processing.</p>
            </list-item>
          </list>
        </sec>
        <sec id="blogs">
          <title>11.1.8 Blogs</title>
          <list list-type="bullet">
            <list-item>
              <p><bold>The Gradient</bold> - A blog that critiques and
          contextualises new developments in AI and machine
          learning.</p>
            </list-item>
            <list-item>
              <p><bold>Distill.pub</bold> - Offers visually rich and
          in-depth explanations of machine learning concepts, accessible
          to a broader audience.</p>
            </list-item>
            <list-item>
              <p>“The Illustrated Transformer” by Jay Alammar</p>
            </list-item>
            <list-item>
              <p>“Hugging Face’s Transformers: State of the Art Natural
          Language Processing” by Patrick von Platen</p>
            </list-item>
            <list-item>
              <p>“The Annotated Transformer” by Alexander Rush</p>
            </list-item>
          </list>
        </sec>
        <sec id="open-source-libraries">
          <title>11.1.9 Open-Source Libraries</title>
          <list list-type="bullet">
            <list-item>
              <p>Hugging Face Transformers:
          https://huggingface.co/transformers/</p>
            </list-item>
            <list-item>
              <p>spaCy: https://spacy.io/</p>
            </list-item>
            <list-item>
              <p>NLTK (Natural Language Toolkit): https://www.nltk.org/</p>
            </list-item>
          </list>
        </sec>
        <sec id="online-platforms">
          <title>11.1.10 Online Platforms</title>
          <list list-type="bullet">
            <list-item>
              <p>OpenAI Playground: https://openai.com/playground/</p>
            </list-item>
            <list-item>
              <p>Anthropic Playground: https://www.anthropic.com/</p>
            </list-item>
            <list-item>
              <p>Cohere Playground: https://www.cohere.com/</p>
            </list-item>
          </list>
          <p>These resources provide a variety of perspectives and depth,
      catering to different levels of expertise and areas of interest in
      the field of LLMs. Whether you’re a beginner looking to get
      started or an advanced practitioner seeking to innovate, these
      resources can enhance your understanding and skills.</p>
        </sec>
      </sec>
    </sec>
    <sec id="glossary">
      <title>12 Glossary</title>
      <sec id="abstract-reasoning-1">
        <title>12.1 <bold>Abstract Reasoning</bold></title>
        <p>The ability to understand complex concepts and apply logical
    thinking to new problems without relying solely on factual
    knowledge.</p>
      </sec>
      <sec id="bias">
        <title>12.2 <bold>Bias</bold></title>
        <p>The tendency of an AI system to produce outputs that discriminate
    against certain groups due to biases present in the training
    data.</p>
      </sec>
      <sec id="chunking">
        <title>12.3 <bold>Chunking</bold></title>
        <p>The process of dividing a long document into smaller, discrete
    pieces that fit within an LLM’s maximum token count.</p>
      </sec>
      <sec id="context-length">
        <title>12.4 <bold>Context Length</bold></title>
        <p>The maximum number of tokens an LLM can process in a single
    prompt.</p>
      </sec>
      <sec id="creative-text-generation-1">
        <title>12.5 <bold>Creative Text Generation</bold></title>
        <p>The ability of an LLM to generate imaginative and creative forms
    of text, such as stories or poems.</p>
      </sec>
      <sec id="embedding">
        <title>12.6 <bold>Embedding</bold></title>
        <p>A vector representation of text that captures its semantic
    essence.</p>
      </sec>
      <sec id="explainability">
        <title>12.7 <bold>Explainability</bold></title>
        <p>The ability to interpret and explain the outputs of an AI system,
    particularly important for building trust and accountability.</p>
      </sec>
      <sec id="fairness">
        <title>12.8 <bold>Fairness</bold></title>
        <p>Ensuring that AI systems treat individuals and groups equitably,
    without discrimination based on protected characteristics.</p>
      </sec>
      <sec id="fine-tuning">
        <title>12.9 <bold>Fine-Tuning</bold></title>
        <p>The process of further training an LLM on a specific dataset or
    task to optimise its performance for that particular use case.</p>
      </sec>
      <sec id="hallucination">
        <title>12.10 <bold>Hallucination</bold></title>
        <p>The generation of plausible-sounding but factually incorrect
    information by an LLM.</p>
      </sec>
      <sec id="large-language-model-llm">
        <title>12.11 <bold>Large Language Model (LLM)</bold></title>
        <p>An AI model, such as OpenAI’s GPT series, that is capable of
    understanding and generating human-like text.</p>
      </sec>
      <sec id="prompt-engineering">
        <title>12.12 <bold>Prompt Engineering</bold></title>
        <p>The art of crafting effective prompts to elicit desired outputs
    from an LLM.</p>
      </sec>
      <sec id="question-answering-1">
        <title>12.13 <bold>Question Answering</bold></title>
        <p>The ability of an LLM to comprehend a body of text and provide
    answers to questions based on that text.</p>
      </sec>
      <sec id="raptor-recursive-abstractive-processing-for-tree-organised-retrieval">
        <title>12.14 <bold>RAPTOR (Recursive Abstractive Processing for
    Tree-Organised Retrieval)</bold></title>
        <p>A hierarchical retrieval method that enhances LLMs by providing
    access to relevant information at different levels of
    specificity.</p>
      </sec>
      <sec id="rag-retrieval-augmented-generation">
        <title>12.15 <bold>RAG (Retrieval-Augmented
    Generation)</bold></title>
        <p>A technique that combines the generative capabilities of LLMs
    with a retrieval mechanism to fetch relevant external information
    and generate more informed responses.</p>
      </sec>
      <sec id="sentiment-analysis-1">
        <title>12.16 <bold>Sentiment Analysis</bold></title>
        <p>The process of determining the emotional tone behind a series of
    words, useful for understanding attitudes, opinions, and emotions
    expressed in text.</p>
      </sec>
      <sec id="text-classification-1">
        <title>12.17 <bold>Text Classification</bold></title>
        <p>The process of categorising text into predefined categories, such
    as topics or genres.</p>
      </sec>
      <sec id="text-chunking">
        <title>12.18 <bold>Text Chunking</bold></title>
        <p>The process of dividing a long document into smaller, more
    manageable pieces for processing by an LLM.</p>
      </sec>
      <sec id="token">
        <title>12.19 <bold>Token</bold></title>
        <p>A fundamental unit of text processed by an LLM, typically a word
    or subword.</p>
      </sec>
      <sec id="vector-database">
        <title>12.20 <bold>Vector Database</bold></title>
        <p>A database that stores data as vectors of real numbers, which
    represent different features or aspects of the data items.</p>
      </sec>
    </sec>
  </body>
  <back>
</back>
</article>
